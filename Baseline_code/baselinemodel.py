# -*- coding: utf-8 -*-
"""baselineModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lx-VRe98Wow1-K8-oIcfENoOvgVJ87Dm
"""

from google.colab import drive
drive.mount('/content/drive')

import csv
import nltk
from nltk.corpus import stopwords
import re
from nltk.stem import PorterStemmer
import string
from nltk import word_tokenize
import gensim 
from gensim.models import Word2Vec 
nltk.download('punkt')
nltk.download('wordnet')
nltk.download("stopwords")
import numpy as np
import pandas as pd
import pickle

original=[]
changed = []
with open('/content/drive/MyDrive/IR Project/archive/Questions.csv', encoding="latin1") as csvfile ,open('/content/drive/MyDrive/IR Project/archive/Output.csv' ,'w' ,newline='') as myfile:
  readCSV = csv.reader(csvfile, delimiter=',' )
  writeCSV = csv.writer(myfile , delimiter = ',')
  next(readCSV, None) 
  for row in readCSV:
    doc_id = row[0];
    title = row[5];
    original.append(title)
    data = preprocessing(title)
    replace = row[5].replace(title,data)
    row[5] = replace
    changed.append(row[5])
    writeCSV.writerow({row[5]})

for doc in questions:
  w =doc.split()
  print(w)

stop_words=stopwords.words('english')
stemming = PorterStemmer()

def preprocessing(text):
  text = text.lower()
  text = text.replace("?"," ?")
  text = text.split()
  words = [stemming.stem(w) for w in text if not w in stop_words]
  sentence  = ' '.join(words)
  return sentence

with open('/content/drive/MyDrive/IR Project/archive/Output.csv' ,'w' ,newline='') as myfile:
  writeCSV = csv.writer(myfile , delimiter = ',')
  for w in changed:
    writeCSV.writerow({ w })

changed = np.array(changed)

np.savetxt("Output.csv",  
           changed, 
           delimiter =", ",  
           fmt ='% s')

changed = list(changed)

changed = []
with open('/content/drive/MyDrive/IR project/Output.csv', encoding="latin1") as csvfile:
  readCSV = csv.reader(csvfile, delimiter=',' )
  for row in readCSV:
    title = row[0];
    changed.append(title)

!pip install gensim

from gensim.models.keyedvectors import KeyedVectors

model_w2v = KeyedVectors.load_word2vec_format("/content/drive/MyDrive/IR Project/Pre-Trained model/SO_vectors_200.bin", binary=True)

filename = "/content/drive/MyDrive/IR Project/W2V_model.sav"
pickle.dump(model_w2v, open(filename, 'wb'))

model_w2v = pickle.load(open("/content/drive/MyDrive/IR project/W2V_model.sav", 'rb'))

#model_w2v = gensim.models.Word2Vec(words , size=300, window=10 , negative=25)
#model_w2v.train(words, total_examples=len(words), epochs=20)

from sklearn.feature_extraction.text import TfidfVectorizer
def tfidfModel(sentences):
  tfidf_model = TfidfVectorizer()
  tfidf_model.fit(sentences)
  word_Tfidf_Dict = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))
  tfidf_features = tfidf_model.get_feature_names()
  return word_Tfidf_Dict, tfidf_features

dictionary, features = tfidfModel(changed)

def tfidfWeightedWord2vec(sentences, dictionary, features):
  x_train = []
  w2v_vocab = list(model_w2v.wv.vocab)
  for sent in sentences:
    #print(sent)
    sent_vec = np.zeros(200)
    ti_idf_score_sum = 0 
    wordlist = sent.split(" ")
    #print(wordlist)
    for word in wordlist:
      if word in w2v_vocab and word in features:
        word_vec = model_w2v.wv[word]
        tf_idf_score = dictionary[word]*(wordlist.count(word)/len(wordlist))
        sent_vec += (word_vec * tf_idf_score)
        ti_idf_score_sum += tf_idf_score 
    if (ti_idf_score_sum !=0):
      sent_vec = sent_vec/ti_idf_score_sum
    x_train.append(sent_vec) 
  return x_train

sliced_data = changed

x_train = tfidfWeightedWord2vec(sliced_data, dictionary, features)

df = pd.DataFrame(x_train)
df.shape

df.to_csv("/content/drive/MyDrive/IR project/combined/w2vembeddings_full_combined.csv" ,header=None, index=False)

def readCsv(path):
  with open(path, encoding="latin1") as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',' )
    test_data_embeddings = []
    for row in readCSV:
      embedd = []
      for i in range(0,len(row)):
        embedd.append(row[i])
      embedd = [(float)(word_embedding) for word_embedding in embedd]
      test_data_embeddings.append(embedd)   
  return test_data_embeddings

def getTitleList():
  titleList = []
  with open('/content/drive/MyDrive/IR project/Questions.csv', encoding="latin1") as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',' )
    next(readCSV, None) 
    for row in readCSV:
      title = row[5];
      titleList.append(title)
  return titleList

titlelist = getTitleList()

import numpy as np
from scipy.spatial import distance
def calCosSim(test_query_embedding, embeddings, titlelist):
  test_query_embeddingArr = np.array(test_query_embedding)
  embeddingsArr = np.array(embeddings)
  result = distance.cdist(test_query_embeddingArr, embeddingsArr, "cosine")
  final_res = 1-result[0]
  final_list = list(zip(final_res, titlelist))
  final_list.sort(reverse=True)
  return final_list

train_embeddings = readCsv("/content/drive/MyDrive/IR project/combined/w2vembeddings_full_combined.csv")

print(train_embeddings[0])

queries = ['insert query in sql table', 'tags in html and css', 'concatenate a string in C#' ]
for query in queries:
  processed_test_query = preprocessing(query)
  print("query: ", query)
  test_query_embeddings = tfidfWeightedWord2vec([processed_test_query], dictionary, features)
  final_list = calCosSim(test_query_embeddings, train_embeddings, titlelist)
  print("relevant result with score:")
  for f in final_list[:10]:
    print(f[0]," : ", f[1])
  print("\n")