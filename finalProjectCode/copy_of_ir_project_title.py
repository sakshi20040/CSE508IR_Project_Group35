# -*- coding: utf-8 -*-
"""Copy of IR_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T5MMn_brCcTQzbjgpU9q_SnF380SfSsE

Group 35 \\
Project code
"""

#mounting 
from google.colab import drive
drive.mount('/content/drive')

!pip install contractions

import csv
import nltk
from nltk.corpus import stopwords
import re
from nltk.stem import PorterStemmer
import string
from nltk import word_tokenize
import gensim 
from gensim.models import Word2Vec 
nltk.download('punkt')
nltk.download('wordnet')
nltk.download("stopwords")
import numpy as np
import pandas as pd
import pickle
import contractions
from bs4 import BeautifulSoup
import tensorflow as tf
import tensorflow_hub as hub
import ast


from tqdm import tqdm_notebook

stop_words=stopwords.words('english')
stemming = PorterStemmer()

def preprocessingQT(text):
  #convert to lowercase
  text = text.lower()
  #expand contractions
  text = contractions.fix(text)
  text = text.replace("?"," ?")
  #split to tokens
  text = text.split()
  #stopward removal and stemming
  words = [stemming.stem(w) for w in text if not w in stop_words]
  sentence  = ' '.join(words)
  return sentence

def preprocessingQB(text):
  #convert to lowercase
  text = text.lower()
  #expand contractions
  text = contractions.fix(text)
  #replace \n with space
  text = re.sub("\n"," ", text)
  #replace quotes with space
  text = re.sub("[\"\']"," ", text)
  #replace codeblocks with space.
  text = re.sub("<pre><code>[A-A a-z 0-9 !\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~]*</code></pre>"," ",text);
  #remove remaining tags.
  soup = BeautifulSoup(text)
  text = soup.get_text()
  #split to tokens
  text = text.split()
  #stopward removal and stemming
  words = [stemming.stem(w) for w in text if not w in stop_words]
  sentence  = ' '.join(words)
  return sentence

original=[]
changed = []
def savePreproccessedData():
  with open('/content/drive/MyDrive/IR project/archive/Questions.csv', encoding="latin1") as csvfile ,open('/content/drive/MyDrive/IR project/archive/finalOutput_soumam_testing_with_title_only1.csv' ,'w' ,newline='') as myfile:
    readCSV = csv.reader(csvfile, delimiter=',' )
    writeCSV = csv.writer(myfile , delimiter = ',')
    next(readCSV, None) 

    #count=0
    for row in tqdm_notebook(readCSV):
      doc_id = row[0];
      title = row[5];
    #   body = row[6];

    #   data = title+" "+ body
      data = title

      p_title = preprocessingQT(title)
    #   p_body = preprocessingQB(body)
      
    #   p_data = p_title+" "+p_body
      p_data = p_title
      
      original.append(data)
      changed.append(p_data)
      writeCSV.writerow((doc_id, p_data))

#Preprocessing the corpus and storing in new file.
savePreproccessedData()

#loading the universal sentence encoder model
use_embeddings = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

embeddings  = []
for c in tqdm_notebook(changed):
    ques_list = [c]
    sent_tfTensor = use_embeddings(ques_list)
    array_embeddings = tf.make_ndarray(tf.make_tensor_proto(sent_tfTensor))
    list_embeddings = array_embeddings.tolist()[0]
    embeddings.append(np.array(list_embeddings))

joblib.dump(embeddings,"/content/drive/MyDrive/IR project/title_embeddings_full" )

#function to create the sentence embeddings and store in a csv file.
def createEmbeddings():
  with open('/content/drive/MyDrive/IR project/archive/finalOutput_soumam_testing_with_title_only.csv', encoding="latin1") as csvfile, open('/content/drive/MyDrive/IR project/sentenceEmbeddings_soumam_testing_with_title_only.csv' ,'w' ,newline='') as myfile:
    readCsv = csv.reader(csvfile, delimiter=',' )
    writeCSV = csv.writer(myfile , delimiter = ',')

    #count =0
    for row in tqdm_notebook(readCsv):
      '''
      if (count==200000):
          break

      count +=1
      '''
      id = row[0]
      ques = row[1]
      ques_list = [ques]
      sent_tfTensor = use_embeddings(ques_list)
      array_embeddings = tf.make_ndarray(tf.make_tensor_proto(sent_tfTensor))
      list_embeddings = array_embeddings.tolist()[0]
      writeCSV.writerow((id, list_embeddings))

createEmbeddings()

def getTitleList():
  titledict = {}
  with open('/content/drive/MyDrive/IR project/Questions.csv', encoding="latin1") as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',' )
    next(readCSV, None) 

    #count=0
    for row in tqdm_notebook(readCSV):
      '''
      if (count==200000):
          break
      '''
      count +=1 

      title = row[5];
      titledict[row[0]] = title
  return titledict

titledict = getTitleList()

#opening the csv file 
filename = open("/content/drive/MyDrive/IR project/sentenceEmbeddings_soumam_testing_with_title_only.csv", 'r')  
# creating dictreader object with col names
file = csv.DictReader(filename, fieldnames=['id', 'vector'])

embeddings = []
idlist = []

count =0
for col in tqdm_notebook(file):
  embeddings.append(ast.literal_eval(col['vector']))
  idlist.append(col['id'])

filename.close()

import joblib

joblib.dump(embeddings,"/content/drive/MyDrive/IR project/embeddingList1_soumam_testing_with_title_only" )
joblib.dump(idlist,"/content/drive/MyDrive/IR project/embeddingList1_soumam_testing_with_title_only_soumam_testing_with_title_only" )

embeddings = joblib.load("/content/drive/MyDrive/IR project/embeddingList1_soumam_testing_with_title_only")

from scipy.spatial import distance
def calCosSim(test_query_embedding, embeddings, idlist):
  test_query_embeddingArr = np.array(test_query_embedding)
  embeddingsArr = np.array(embeddings)
  result = distance.cdist(test_query_embeddingArr, embeddingsArr, "cosine")
  final_res = 1-result[0]
  final_list = list(zip(final_res, idlist))
  final_list.sort(reverse=True)
  return final_list

queries = ['insert query in sql table', 'tags in html and css', 'concatenate a string in C#', 'Sql joins between tables' ,'how to add links in html']
for query in queries:
  processed_test_query = preprocessingQT(query)
  print("query: ", query)
  sent_tfTensor = use_embeddings([processed_test_query])
  array_embeddings = tf.make_ndarray(tf.make_tensor_proto(sent_tfTensor))
  test_query_embeddings  = array_embeddings.tolist()
  #print(test_query_embeddings)
  final_list = calCosSim(test_query_embeddings, embeddings, idlist)
  print("relevant result with score:")
  for f in final_list[:20]:
    print(f[0]," : ", titledict[f[1]])
  print("\n")